{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosyne 2018 Project: Synaptic Computation in the NEF\n",
    "\n",
    "**Preliminary Abstract**:\n",
    "\n",
    "Non-linear interaction in the dendritic tree is known to be an important computational resource in biological neurons. Yet, high-level neural compilers ‒ such as the Neural Engineering Framework (NEF) ‒ neither include non-linear synaptic interaction in their models, nor exploit these interactions systematically. In this study, we extend the NEF towards synaptic computation of non-linear multivariate functions, such as controlled shunting, multiplication, maximum, and the Euclidean norm. We present a theoretical framework that provides sufficient conditions under which non-linear synaptic interaction yields a similar precision compared to traditional NEF methods, while reducing the number of layers, neurons, and latency in the network. The proposed method lends itself to increasing the computational power of neuromorphic hardware systems and improves the NEF's biological plausibility by mitigating one of its long-standing limitations, namely its reliance on linear, current-based synapses. We perform a series of numerical experiments with a conductance-based two-compartment LIF neuron model. Preliminary results show that non-linear interaction of conductance-based synapses is sufficient to compute a wide variety of non-linear functions with performance competitive to using an additional layer of neurons as a non-linearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import numpy as np\n",
    "import nengo\n",
    "nengo.rc.set('decoder_cache', 'enabled', 'False')\n",
    "\n",
    "import scipy.optimize\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical Neuron Simulation\n",
    "\n",
    "The following code implements empirical simulation of a single spiking neuron. This simulation is used to extract the Neuron Response curve, i.e. the average activity of the neuron for a sweep over the excitatory and inhibitory input spike rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Neuron and synapse parameters\n",
    "gC = 100e-9\n",
    "gLDen = 12.5e-9\n",
    "gLSom = 50e-9\n",
    "cmDen = 0.25e-9\n",
    "cmSom = 1.0e-9\n",
    "EE = 20e-3\n",
    "EI = -75e-3\n",
    "EL = -65e-3\n",
    "tau_syn_e = 5e-3\n",
    "tau_syn_i = 10e-3\n",
    "w_syn_e = 100e-9\n",
    "w_syn_i = 100e-9\n",
    "v_thresh = -50e-3\n",
    "tau_ref = 2e-3\n",
    "tau_rc = cmSom / gLSom\n",
    "\n",
    "# Input rates, convert to average conductances gEs and gIs\n",
    "rEs = np.linspace(0, 7500, 10)\n",
    "rIs = np.linspace(0, 5000, 10)\n",
    "gEs = rEs * w_syn_e\n",
    "gIs = rIs * w_syn_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spike_train(lambda_=0.01, T=10.0):\n",
    "    t = 0\n",
    "    res = []\n",
    "    while t < T:\n",
    "        t += np.random.exponential(lambda_)\n",
    "        res.append(t)\n",
    "    return res\n",
    "\n",
    "def lif_rate(J):\n",
    "    mask = 1 * np.logical_or(\n",
    "            np.logical_and(J > 1, gLSom > 0),\n",
    "            np.logical_and(J < 0, gLSom < 0))\n",
    "    t_spike = -np.log1p(-mask * 1.0 / (mask * J + (1 - mask))) * tau_rc\n",
    "    return mask / (tau_ref + t_spike)\n",
    "\n",
    "def lif_rate_inv(r):\n",
    "    return -1 / (np.exp((r * tau_ref - 1) / (r * tau_rc)) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for Empirical Neuron Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Point Neuron with Conductance-Based Synapses\n",
    "\n",
    "*Note:* This code is only included as a refference. It uses a C++ simulator backend provided by `nengo_conductance_synapses.sim_cond_exp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "\n",
    "def point_lif_cond_exp():\n",
    "    from nengo_conductance_synapses import sim_cond_exp\n",
    "\n",
    "    # Define all simulation parameters (these are the default values)\n",
    "    task = sim_cond_exp.task(\n",
    "             cm=cmSom,\n",
    "             g_leak=gLSom,\n",
    "             v_thresh=v_thresh,\n",
    "             v_rest=EL,\n",
    "             v_reset=EL,\n",
    "             tau_refrac=tau_ref,\n",
    "             tau_syn_e=tau_syn_e,\n",
    "             w_syn_e=w_syn_e,\n",
    "             e_rev_syn_e=EE,\n",
    "             tau_syn_i=tau_syn_i,\n",
    "             w_syn_i=w_syn_i,\n",
    "             e_rev_syn_i=EI,\n",
    "             repeat=10,\n",
    "             T=1.0);\n",
    "\n",
    "    # Create a SimCondExp object and submit the task, wait until the simulation\n",
    "    # has finished\n",
    "    output_rates = np.zeros((len(rIs), len(rEs)))\n",
    "    with sim_cond_exp.SimCondExp() as sim:\n",
    "        def make_callback(i, j):\n",
    "            def callback(_, rates):\n",
    "                output_rates[j, i] = np.mean(rates)\n",
    "            return callback\n",
    "        for i, rE in enumerate(rEs):\n",
    "            for j, rI in enumerate(rIs):\n",
    "                task[\"input\"][\"rate_e\"] = rE\n",
    "                task[\"input\"][\"rate_i\"] = rI\n",
    "                sim.submit(task, make_callback(i, j))\n",
    "    return output_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LIF Neuron with two Compartments and Conductance-Based Synapses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%script false \n",
    "\n",
    "def simulate_two_compartement_lif(exc_spike_rate,\n",
    "                                  inh_spike_rate,\n",
    "                                  T=1.0,\n",
    "                                  dt=1e-5):\n",
    "    # Create the output arrays, including voltage and current traces\n",
    "    ts = np.arange(0, T, dt)\n",
    "    n_samples = len(ts)\n",
    "    vDens, vSoms, iSyns, gEs, gIs = np.empty((5, n_samples))\n",
    "    spikes = []\n",
    "\n",
    "    # Generate the excitatory and inhibitory input spike trains\n",
    "    exc_spikes = spike_train(1.0 / (exc_spike_rate + 1e-3), T)\n",
    "    inh_spikes = spike_train(1.0 / (inh_spike_rate + 1e-3), T)\n",
    "    gE = 0.0\n",
    "    gI = 0.0\n",
    "\n",
    "    # Iterate over all timesteps\n",
    "    vDen = vSom = EL\n",
    "    tRef = 0.0\n",
    "    for i, t in enumerate(ts):\n",
    "        # Synaptic dynamics\n",
    "        gE += -dt * gE / tau_syn_e\n",
    "        gI += -dt * gI / tau_syn_i\n",
    "        while (len(exc_spikes) > 0 and t > exc_spikes[0]):\n",
    "            gE += w_syn_e\n",
    "            del exc_spikes[0]\n",
    "        while (len(inh_spikes) > 0 and t > inh_spikes[0]):\n",
    "            gI += w_syn_i\n",
    "            del inh_spikes[0]\n",
    "\n",
    "        # Calculate the synaptic current from the compartemental voltages in the\n",
    "        # last timestep (this is what couples the compartements)\n",
    "        iSyn = (vDen - vSom) * gC\n",
    "\n",
    "        # Advance the simulation independently for the dendritic and somatic\n",
    "        # compartement\n",
    "        vDen += dt * ((EE - vDen) * gE + (EI - vDen) * gI +\n",
    "                      (EL - vDen) * gLDen - iSyn) / cmDen\n",
    "        vSom += dt * ((EL - vSom) * gLSom + iSyn) / cmSom\n",
    "\n",
    "        # Implement the spike generation mechanism in the soma\n",
    "        if vSom > v_thresh:\n",
    "            tRef = tau_ref\n",
    "            spikes.append(t)\n",
    "        if tRef > 0.0:\n",
    "            vSom = EL\n",
    "            tRef = np.maximum(0.0, tRef - dt)\n",
    "\n",
    "        # Record the somatic and dendritic voltages as well as the copuling\n",
    "        # current\n",
    "        vDens[i] = vDen\n",
    "        vSoms[i] = vSom\n",
    "        iSyns[i] = iSyn\n",
    "        gEs[i] = gE\n",
    "        gIs[i] = gI\n",
    "\n",
    "    # Use the median spike distance as frequency\n",
    "    n_spikes = len(spikes)\n",
    "    f_spike = n_spikes / T\n",
    "    spikes = np.array(spikes)\n",
    "    if n_spikes > 2:\n",
    "        f_spike = 1 / np.median(spikes[1:] - spikes[:-1])\n",
    "\n",
    "    return ts, vDens, vSoms, iSyns, gEs, gIs, f_spike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "\n",
    "def two_comp_lif_cond_exp(T=1.0, dt=1e-5, repeat=10):\n",
    "    mrEs, mrIs = np.meshgrid(rEs, rIs)\n",
    "    it = np.nditer(\n",
    "            [mrEs, mrIs, None],\n",
    "            [],\n",
    "            [['readonly'], ['readonly'], ['writeonly', 'allocate']])\n",
    "\n",
    "    for (rE, rI, rate) in it:\n",
    "        f_spikes = np.zeros(repeat)\n",
    "        for i in range(repeat):\n",
    "            _, _, _, _, _, _, f_spikes[i] = simulate_two_compartement_lif(rE, rI, T, dt)\n",
    "        rate[...] = np.mean(f_spikes)\n",
    "    output_rates = it.operands[2]\n",
    "    return output_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron Response Curve from Empirical Simulation\n",
    "\n",
    "Run the actual neuron simulation. We will fit this to a model later, so we don't have to constantly perfrom numerical simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "\n",
    "# Executing this code will take some time...\n",
    "#output_rates = point_lif_cond_exp();\n",
    "output_rates = two_comp_lif_cond_exp();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "contour = ax.contourf(rEs, rIs, output_rates, 10, cmap='viridis')\n",
    "levels = contour.levels\n",
    "\n",
    "ax.contour(rEs, rIs, output_rates, levels, colors=[\"w\"], linestyles=[(0, (1, 3))])\n",
    "ax.set_xlabel('Excitatory rate [$s^{-1}]$')\n",
    "ax.set_ylabel('Inhibitory rate [$s^{-1}]$')\n",
    "ax.set_title('Output Spike Rate')\n",
    "\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "\n",
    "fig.colorbar(contour, cax=cax, orientation='vertical');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the synaptic portion of the neuron from the neuron response curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "\n",
    "j_from_rate = lif_rate_inv(output_rates + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "contour = ax.contourf(rEs, rIs, j_from_rate, 10, cmap='viridis')\n",
    "levels = contour.levels\n",
    "\n",
    "ax.contour(rEs, rIs, j_from_rate, levels, colors=[\"w\"], linestyles=[(0, (1, 3))])\n",
    "ax.set_xlabel('Excitatory rate [$s^{-1}$]')\n",
    "ax.set_ylabel('Inhibitory rate [$s^{-1}$]')\n",
    "ax.set_title('Equivalent Somatic Current')\n",
    "\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "fig.colorbar(contour, cax=cax, orientation='vertical');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Neuron Response Curve to a Rational Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "j_model_fun = lambda w, gE, gI: (w[0] + w[1] * gE + w[2] * gI) / (w[3] + w[4] * gE + w[5] * gI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "\n",
    "valid = output_rates.flatten() > 1\n",
    "mgEs, mgIs = np.meshgrid(gEs, gIs)\n",
    "gE_train = mgEs.flatten()[valid]\n",
    "gI_train = mgIs.flatten()[valid]\n",
    "tar = j_from_rate.flatten()[valid]\n",
    "\n",
    "err = lambda w: np.sqrt(np.mean((j_model_fun(w, gE_train, gI_train) - tar) ** 2))\n",
    "\n",
    "w0 = np.array((gC * (v_thresh - EL) + gC, EE + 1, EI + 1, gC, 1, 1))\n",
    "w0[0:3] *= 500\n",
    "res = scipy.optimize.basinhopping(err, w0)\n",
    "w = res.x / res.x[3] # Reduce DOFs\n",
    "\n",
    "print(\"Weights:\\n\", w)\n",
    "print(\"Error:\\n\", res.fun)\n",
    "\n",
    "j_model = j_model_fun(w, mgEs.flatten(), mgIs.flatten()).reshape(mgEs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Result of all the above calculations\n",
    "w =  np.array((\n",
    "     3.51649857e+00,\n",
    "     2.33295706e+05,\n",
    "    -1.39254970e+05,\n",
    "     1.00000000e+00,\n",
    "     2.24264021e+04,\n",
    "     4.21362391e+04))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "contour = ax.contourf(gEs * 1e6, gIs * 1e6, j_model.reshape(mgEs.shape), levels, cmap='viridis')\n",
    "ax.contour(gEs * 1e6, gIs * 1e6, j_from_rate, levels, linestyles=('--'), colors=('white'))\n",
    "ax.set_xlabel('Excitatory conductance [$\\mathrm{\\mu S}$]')\n",
    "ax.set_ylabel('Inhibitory conductance [$\\mathrm{\\mu S}$]')\n",
    "ax.set_title('Somatic Current Fit (colour) vs. to Emprical (dashed)')\n",
    "\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "fig.colorbar(contour, cax=cax, orientation='vertical');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network setup\n",
    "\n",
    "Here I manually generate standard parameters for a network of LIF neurons with three populations à 100 neurons each. The first two populations project onto the third population, but instead of computing the sum of the values represented by the first two populations we'd like to compute the product of the two values. In particular, I'm only concerned about positive input values, that is the product as operator $\\cdot : [0, 1] \\times [0, 1] \\longrightarrow [0, 1]$.\n",
    "\n",
    "The following code computes a set of parameters, tuning curves, and decoders for each population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_tuning_curves(N=100, max_rate=200, n=3):\n",
    "    max_rates = np.random.uniform(max_rate / 2, max_rate, (n, N))\n",
    "    intercepts = np.random.uniform(-0.95, 0.95, (n, N))\n",
    "    encoders = np.random.choice([-1, 1], (n, N))\n",
    "    neuron = nengo.LIF(tau_rc=cmSom / gLSom, tau_ref=tau_ref)\n",
    "    gain, bias = neuron.gain_bias(max_rates, intercepts)\n",
    "\n",
    "    # Calculate population tuning curves and decoders\n",
    "    res = 100\n",
    "    xs = np.linspace(-1, 1, res)\n",
    "    As = np.zeros((n, res, N))\n",
    "    decoders = np.zeros((n, N))\n",
    "    for ni in range(n):\n",
    "        for Ni in range(N):\n",
    "            As[ni, :, Ni] = lif_rate(\n",
    "                (xs * encoders[ni, Ni] * gain[ni, Ni] + bias[ni, Ni]))\n",
    "            decoders[ni] = nengo.solvers.LstsqL2()(As[ni], xs)[0]\n",
    "\n",
    "    return n, xs, As, gain, bias, encoders, decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "\n",
    "# Plot some tuning curves as an example of what they look like\n",
    "n, xs, As, gain, bias, encoders, decoders = generate_tuning_curves()\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 3))\n",
    "for ni in range(n):\n",
    "    axs[ni].plot(xs, As[ni])\n",
    "    if ni < 2:\n",
    "        axs[ni].set_title(\"Pre population ({:d})\".format(ni + 1))\n",
    "    else:\n",
    "        axs[ni].set_title(\"Post population\")\n",
    "    axs[ni].set_xlabel(\"Value $x$\")\n",
    "    axs[ni].set_ylabel(\"Firing rate\");\n",
    "    axs[ni].set_xlim(-1, 1)\n",
    "    axs[ni].set_ylim(0, 200)\n",
    "    axs[ni].get_yaxis().set_visible(ni == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating “Training” Data\n",
    "\n",
    "In order to train the network, let's first generate some reference data. To this end I uniformly sample the 2D-input space and compute the desired product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_training_data(h=lambda x, y: x * y, res=20):\n",
    "    # Generate sample points on a uniform grid\n",
    "    xs = np.linspace(0, 1, res)\n",
    "    data_in_x, data_in_y = np.meshgrid(xs, xs)\n",
    "    data_in = np.array((data_in_x.flatten(), data_in_y.flatten()))\n",
    "\n",
    "    # Generate the target data\n",
    "    data_tar = h(data_in[0], data_in[1])\n",
    "\n",
    "    return data_in, data_tar\n",
    "\n",
    "def target_currents(data_tar, gain, bias, encoders):\n",
    "    return data_tar[:, None] @ (gain * encoders)[None, :] + bias[None, :]\n",
    "\n",
    "def population_activities(data_in, data_tar, gain, bias, encoders):\n",
    "    assert gain.shape == bias.shape\n",
    "    assert gain.shape[0] == encoders.shape[0]\n",
    "\n",
    "    n = 3\n",
    "    N = gain.shape[1]\n",
    "    n_samples = data_in.shape[1]\n",
    "\n",
    "    neuron = nengo.LIF(tau_rc=cmSom / gLSom, tau_ref=tau_ref)\n",
    "    A = np.empty((n, N, n_samples))\n",
    "    for Ni in range(N):\n",
    "        A[0, Ni] = neuron.rates(data_in[0] * encoders[0, Ni], gain[0, Ni], bias[0, Ni])\n",
    "        A[1, Ni] = neuron.rates(data_in[1] * encoders[1, Ni], gain[1, Ni], bias[1, Ni])\n",
    "        A[2, Ni] = neuron.rates(data_tar * encoders[2, Ni], gain[2, Ni], bias[2, Ni])\n",
    "    Apre = (A[0:2].reshape((2*N, n_samples))).T\n",
    "    Apost = (A[2]).T\n",
    "    return Apre, Apost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "\n",
    "# Plot the training data and population activities for some function h\n",
    "h = lambda x, y: 0.5 * (x + y)\n",
    "\n",
    "data_in, data_tar = generate_training_data(h)\n",
    "Jpost = target_currents(data_tar, gain[2], bias[2], encoders[2])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(Jpost.T, vmin=0)\n",
    "ax.set_title('Target currents per sample/neuron')\n",
    "ax.set_xlabel(\"Sample number\")\n",
    "ax.set_ylabel(\"Neuron number\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(data_in[0], label=\"First input dim.\")\n",
    "ax.plot(data_in[1], label=\"Second input dim.\")\n",
    "ax.plot(data_tar, linewidth=2, color='k', linestyle=(1, (1, 1)), label=\"Target output\")\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.2),\n",
    "          ncol=3)\n",
    "ax.set_xlabel(\"Sample number\")\n",
    "ax.set_ylabel(\"Value\")\n",
    "ax.set_title(\"Input and output data in value space\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve for Weight Matrices\n",
    "\n",
    "Here, I use gradient descent with enforced non-negativity to solve for excitatory and inhibitory weight matrices $W_\\mathrm{E}$ and $W_\\mathrm{I}$ with size $200 \\times 100$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def solve_conductance_weight_matrices(Apre, Jpost, j_model_weights):\n",
    "    assert Apre.shape[0] == Jpost.shape[0]\n",
    "    m = Apre.shape[0]\n",
    "    Npre = Apre.shape[1]\n",
    "    Npost = Jpost.shape[1]\n",
    "\n",
    "    a0, a1, a2, b0, b1, b2 = j_model_weights\n",
    "    flt_best_err = None\n",
    "    best_errs, errs = np.ones((2, Npost)) * np.inf\n",
    "    WE, WI, best_WI, best_WE = np.zeros((4, Npre, Npost))\n",
    "    WE, WI = np.ones((2, Npre, Npost)) * 2e-2\n",
    "    gE, gI = Apre @ WE, Apre @ WI\n",
    "    lambda_ = 0.1\n",
    "    reg = 1e-3\n",
    "\n",
    "    # Do not spend energy on optimizing for negative currents\n",
    "    Jpost_non_neg = np.copy(Jpost)\n",
    "    Jpost_non_neg[Jpost < 0] = -1\n",
    "\n",
    "    # Code for solving a weight matrix for the given target function\n",
    "    def gradient_descent_step(WE, WI):\n",
    "        eta = lambda_ / m\n",
    "        for i in range(Npost):\n",
    "            for j in np.random.permutation(m):\n",
    "                num = (a0 + a1 * Apre[j, :] @ WE[:, i] + a2 * Apre[j, :] @ WI[:, i])\n",
    "                den = (b0 + b1 * Apre[j, :] @ WE[:, i] + b2 * Apre[j, :] @ WI[:, i])\n",
    "                Jcur = num / den\n",
    "                Jerr = Jpost_non_neg[j, i] - Jcur\n",
    "\n",
    "                # Do not count negative outputs as error\n",
    "                # if the target is negative as well\n",
    "                if Jcur < 0 and Jpost_non_neg[j, i] < 0:\n",
    "                    Jerr = 0\n",
    "\n",
    "                # Add a L2 regularisation term\n",
    "                Jerr += reg * (np.sum(WE[:, i] ** 2) + np.sum(WI[:, i]) ** 2)\n",
    "\n",
    "                # Calculate the gradient\n",
    "                WE_step = Jerr * (- (Apre[j, :] * a1) / den\n",
    "                                  + (num * Apre[j, :] * b1) / (den ** 2) + 2 * reg * WE[:, i])\n",
    "                WI_step = Jerr * (- (Apre[j, :] * a2) / den\n",
    "                                  + (num * Apre[j, :] * b2) / (den ** 2) + 2 * reg * WE[:, i])\n",
    "\n",
    "                # Descent the gradient!\n",
    "                WE[:, i] -= eta * WE_step\n",
    "                WI[:, i] -= eta * WI_step\n",
    "\n",
    "                # Make sure the weights are not exorbitantly large\n",
    "                WE[:, i] = np.clip(WE[:, i], None, 1e3)\n",
    "                WI[:, i] = np.clip(WI[:, i], None, 1e3)\n",
    "\n",
    "        return WE, WI\n",
    "                \n",
    "\n",
    "#    def calculate_tar_for_gE_given_gI(J, gI):\n",
    "#        return (J * (b0 + b2 * gI) - (a0 + a2 * gI)) / (a1 - J * b1)\n",
    "\n",
    "#    def calculate_tar_for_gI_given_gE(J, gE):\n",
    "#        return (J * (b0 + b1 * gE) - (a0 + a1 * gE)) / (a2 - J * b2)\n",
    "\n",
    "    def current_Jpost(WE, WI):\n",
    "        num = (a0 + a1 * Apre @ WE + a2 * Apre @ WI)\n",
    "        den = (b0 + b1 * Apre @ WE + b2 * Apre @ WI)\n",
    "        return num / den\n",
    "\n",
    "    # Alternating optimization of the excitatory and inhibitory weights\n",
    "    i = 0\n",
    "    while True:\n",
    "        # Calculate the error. It is ok if negative target currents are\n",
    "        # just negative in the optimized version.\n",
    "        Jpost_opt = current_Jpost(WE, WI)\n",
    "        Jpost_opt[np.logical_and(Jpost_opt < 0, Jpost_non_neg < 0)] = -1\n",
    "        errs = np.sqrt(np.mean((Jpost_opt - Jpost_non_neg) ** 2, axis=0))\n",
    "        # TODO: Add regularisation\n",
    "\n",
    "        # Keep the best columns in WE, WI\n",
    "        better = errs < best_errs\n",
    "        best_errs[better] = errs[better]\n",
    "        best_WE[:, better] = WE[:, better]\n",
    "        best_WI[:, better] = WI[:, better]\n",
    "\n",
    "        # Optimization controller -- measure the current error and abort once\n",
    "        # the error no longer improves\n",
    "        err = np.sqrt(np.mean(errs**2))\n",
    "        best_err = np.sqrt(np.mean(best_errs**2))\n",
    "        if flt_best_err is None:\n",
    "            flt_best_err = best_err\n",
    "        else:\n",
    "            flt_best_err = best_err * 0.5 + flt_best_err * 0.5\n",
    "            sys.stdout.write((\"Current error: {:0.4f} \" +\n",
    "                              \"Best error: {:0.4f} \" +\n",
    "                              \"ΔE: {:0.4f} \" +\n",
    "                              \"∑|WE|: {:0.2f} \" +\n",
    "                              \"∑|WI|: {:0.2f}     \\r\").format(\n",
    "                err,\n",
    "                best_err,\n",
    "                flt_best_err - best_err,\n",
    "                np.sum(np.abs(WE)) / (np.prod(WE.shape)),\n",
    "                np.sum(np.abs(WI)) / (np.prod(WI.shape))))\n",
    "            sys.stdout.flush()\n",
    "            if (i > 4 / lambda_):\n",
    "                if (np.abs(flt_best_err - best_err) < 1e-4 * lambda_):\n",
    "                    print(\"\\nError not significantly decreasing, done.\")\n",
    "                    break\n",
    "\n",
    "        # Add some noise to shake things up a little...\n",
    "        WE = WE + np.random.normal(0, 1, (Npre, Npost)) * 1e-3\n",
    "        WI = WI + np.random.normal(0, 1, (Npre, Npost)) * 1e-3\n",
    "\n",
    "        # Perform a single gardient descent step\n",
    "        gradient_descent_step(WE, WI)\n",
    "\n",
    "        # Make sure the weight matrices are non-negative\n",
    "        WE[WE < 0] = 0\n",
    "        WI[WI < 0] = 0\n",
    "\n",
    "\n",
    "        i = i + 1\n",
    "        \n",
    "    print(\"Final error:\", best_err)\n",
    "    print(\"Number of iterations:\", i)\n",
    "    return best_WE, best_WI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "\n",
    "Apre, Apost = population_activities(data_in, data_tar, gain, bias, encoders)\n",
    "\n",
    "WE, WI = solve_conductance_weight_matrices(Apre, Jpost, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "\n",
    "Jpost_opt = j_model_fun(w, Apre @ WE, Apre @ WI)\n",
    "Apost_opt = lif_rate(Jpost_opt)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(Apost_opt @ decoders[2], linewidth=3.0, label=\"Decoded\")\n",
    "ax.plot(data_tar, 'k', linewidth=1.0, linestyle=(1, (1, 1)), label=\"Target\", alpha=0.5)\n",
    "ax.plot(data_tar, 'w', linewidth=1.0, linestyle=(0, (1, 1)), alpha=0.5)\n",
    "ax.set_xlabel('Sample number')\n",
    "ax.set_ylabel('Decoded value $\\\\phi(x, y)$')\n",
    "\n",
    "rmse = np.sqrt(np.mean((Apost_opt @ decoders[2] - data_tar)**2))\n",
    "ax.set_title('RMSE: {:0.4f}'.format(rmse));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "\n",
    "ni = [19] # Which post-neurons to inspect\n",
    "\n",
    "fig, axs = plt.subplots(2, figsize=(6, 12))\n",
    "axs[0].plot(Jpost_opt[:, ni], linewidth=3.0, label=\"Decoded\")\n",
    "axs[0].plot(Jpost[:, ni], 'k', linewidth=1.0, linestyle=(1, (1, 1)), label=\"Target\", alpha=0.5)\n",
    "axs[0].plot(Jpost[:, ni], 'w', linewidth=1.0, linestyle=(0, (1, 1)), alpha=0.5)\n",
    "axs[0].set_title('Target current vs. decoded current')\n",
    "axs[0].set_xlabel('Sample number')\n",
    "axs[0].set_ylabel('Current $J$')\n",
    "axs[0].set_ylim(-1, None)\n",
    "axs[0].legend(loc='best')\n",
    "\n",
    "axs[1].plot((Apre @ WE)[:, ni], label=\"$g_\\mathrm{E}$\", alpha=0.75)\n",
    "axs[1].plot((Apre @ WI)[:, ni], label=\"$g_\\mathrm{I}$\", alpha=0.75)\n",
    "axs[1].set_title('Decoded conductance functions $g_\\mathrm{E}(x, y)$, $g_\\mathrm{I}(x, y)$')\n",
    "axs[1].set_xlabel('Sample number')\n",
    "axs[1].set_ylabel('Conductance $g$')\n",
    "axs[1].legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def benchmark_cond_synapses_no_intermediate(φ, w, N):\n",
    "    # Setup the population tuning curves\n",
    "    n, xs, As, gain, bias, encoders, decoders = generate_tuning_curves(N=N)\n",
    "\n",
    "    # Generate input and ouptut data, translate to pre- and post-synaptic activities\n",
    "    data_in, data_tar = generate_training_data(φ)\n",
    "    Apre, _ = population_activities(data_in, data_tar, gain, bias, encoders)\n",
    "\n",
    "    # Calculate the target currents\n",
    "    Jpost = target_currents(data_tar, gain[2], bias[2], encoders[2])\n",
    "\n",
    "    # Solve for WE and WI\n",
    "    print(\"Solving for weight matrices...\")\n",
    "    WE, WI = solve_conductance_weight_matrices(Apre, Jpost, w)\n",
    "\n",
    "    # Evaluate the function on a finer grid\n",
    "    print(\"Calculating RMSE...\")\n",
    "    data_test_in, data_test_tar = generate_training_data(φ, res=100)\n",
    "    Apre, Apost_ground_truth = population_activities(data_test_in, data_test_tar, gain, bias, encoders)\n",
    "    Jpost_opt = j_model_fun(w, Apre @ WE, Apre @ WI)\n",
    "    Apost_opt = lif_rate(Jpost_opt)\n",
    "\n",
    "    # Decode both Apost_truth and Apost_opt\n",
    "    post_ground_truth_decoded = Apost_ground_truth @ decoders[2]\n",
    "    post_opt_decoded = Apost_opt @ decoders[2]\n",
    "\n",
    "    # Calculate the RMSE\n",
    "    rmse_ground_truth = np.sqrt(np.mean((post_ground_truth_decoded - data_test_tar) ** 2))\n",
    "    rmse_opt = np.sqrt(np.mean((post_opt_decoded - data_test_tar) ** 2))\n",
    "\n",
    "    return rmse_opt, rmse_ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def benchmark_cur_synapses_intermediate(φ, N, backprop=False):\n",
    "    # Fetch gain, bias, and encoders to be used for the source and target populations\n",
    "    n, xs, As, gain, bias, encoders, decoders = generate_tuning_curves(N=N)\n",
    "    \n",
    "    # We use LIFRate instead of LIF so that we backprop on the tuning curves,\n",
    "    # and because we don't need the spikes down below.\n",
    "    neuron = nengo.LIFRate(tau_rc=cmSom / gLSom, tau_ref=tau_ref)\n",
    "\n",
    "    # Use the same L2 regularisation as in the gradient descent algorithm\n",
    "    reg = 1e-3\n",
    "    solver = nengo.solvers.LstsqL2(reg=reg)\n",
    "\n",
    "    # Fetch the evaluation points\n",
    "    data_in, data_tar = generate_training_data(φ)\n",
    "\n",
    "    # Build a nengo network\n",
    "    print(\"Solving for weight matrices...\")\n",
    "    net = nengo.Network()\n",
    "    with net:\n",
    "        ens_X = nengo.Ensemble(\n",
    "            N, 1, encoders=encoders[0].reshape(-1, 1),\n",
    "            gain=gain[0], bias=bias[0], neuron_type=neuron,\n",
    "            eval_points=np.unique(data_in[0])[:, None],\n",
    "            label=\"X\")\n",
    "        ens_Y = nengo.Ensemble(\n",
    "            N, 1, encoders=encoders[1].reshape(-1, 1),\n",
    "            gain=gain[1], bias=bias[1], neuron_type=neuron,\n",
    "            eval_points=np.unique(data_in[1])[:, None],\n",
    "            label=\"Y\")\n",
    "        ens_tar = nengo.Ensemble(\n",
    "            N, 1, encoders=encoders[2].reshape(-1, 1),\n",
    "            gain=gain[2], bias=bias[2], neuron_type=neuron,\n",
    "            label=\"Tar\")\n",
    "        ens_inter = nengo.Ensemble(N, 2,\n",
    "            max_rates=nengo.dists.Uniform(100, 200), neuron_type=neuron,\n",
    "            eval_points=data_in.T, radius=np.sqrt(2),\n",
    "            label=\"Inter\")\n",
    "\n",
    "        c_xi = nengo.Connection(ens_X, ens_inter, transform=[[1], [0]], solver=solver)\n",
    "        c_yi = nengo.Connection(ens_Y, ens_inter, transform=[[0], [1]], solver=solver)\n",
    "        c_it = nengo.Connection(ens_inter, ens_tar,\n",
    "                         function=lambda x: φ(x[0], x[1]), solver=solver)\n",
    "\n",
    "    if not backprop:  # least squares\n",
    "        with nengo.Simulator(net, progress_bar=False) as sim:\n",
    "            pass\n",
    "    else:\n",
    "        import nengo_dl\n",
    "        import tensorflow as tf\n",
    "        \n",
    "        with net:\n",
    "            # Switch ens_inter to have soft gradients\n",
    "            # since we are optimizing its encoders.\n",
    "            # The others are LIFRate.\n",
    "            ens_inter.neuron_type = (\n",
    "                nengo_dl.SoftLIFRate(tau_rc=cmSom / gLSom, tau_ref=tau_ref))\n",
    "                \n",
    "            # Only train the weight matrices to ens_inter, its\n",
    "            # biases/encoders, and the decoders from ens_inter.\n",
    "            nengo_dl.configure_settings(trainable=False)\n",
    "            model.config[c_xi].trainable = True\n",
    "            model.config[c_yi].trainable = True\n",
    "            model.config[ens_inter].trainable = True\n",
    "            model.config[c_it].trainable = True\n",
    "\n",
    "        # Use the same training data as Nengo gets.\n",
    "        inputs = {ens_X: data_in[0][:, None, None],\n",
    "                  ens_Y: data_in[1][:, None, None]}\n",
    "        targets = {ens_tar: data_tar[:, None, None]}\n",
    "\n",
    "        opt = tf.train.MomentumOptimizer(\n",
    "            learning_rate=1e-2, momentum=0.9, use_nesterov=True)\n",
    "\n",
    "        with nengo_dl.Simulator(model, minibatch_size=data_in.shape[1]) as sim:\n",
    "            print(\"pre-training mse:\", sim.loss(inputs, targets, \"mse\"))\n",
    "            sim.train(inputs, targets, opt, n_epochs=100)\n",
    "            print(\"post-training mse:\", sim.loss(inputs, targets, \"mse\"))\n",
    "\n",
    "    # Evaluate the function on a finer grid\n",
    "    print(\"Calculating RMSE...\")\n",
    "    data_test_in, data_test_tar = generate_training_data(φ, res=100)\n",
    "    Apre, Apost_ground_truth = population_activities(data_test_in, data_test_tar, gain, bias, encoders)\n",
    "\n",
    "    # Calculate the input current for each neuron in the intermediate population\n",
    "    enc = sim.data[ens_inter].scaled_encoders\n",
    "    Jin_i_x = (enc @ (sim.data[c_xi].weights @ Apre[:, 0:N].T)).T\n",
    "    Jin_i_y = (enc @ (sim.data[c_yi].weights @ Apre[:, N:2*N].T)).T\n",
    "    Jin_i = Jin_i_x + Jin_i_y\n",
    "    Ai = neuron.rates(Jin_i, 1, sim.data[ens_inter].bias)\n",
    "\n",
    "    # Calculate the input current for each neuron in the target population\n",
    "    enc = sim.data[ens_tar].encoders\n",
    "    Jin_t = (enc @ (sim.data[c_it].weights @ Ai.T)).T\n",
    "    Apost_opt = neuron.rates(Jin_t, ens_tar.gain, ens_tar.bias)\n",
    "    \n",
    "    # Decode both Apost_truth and Apost_opt\n",
    "    post_ground_truth_decoded = Apost_ground_truth @ decoders[2]\n",
    "    post_opt_decoded = Apost_opt @ decoders[2]\n",
    "\n",
    "    # Calculate the RMSE\n",
    "    rmse_ground_truth = np.sqrt(np.mean((post_ground_truth_decoded - data_test_tar) ** 2))\n",
    "    rmse_opt = np.sqrt(np.mean((post_opt_decoded - data_test_tar) ** 2))\n",
    "\n",
    "    return rmse_opt, rmse_ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_task(task):\n",
    "    # Reset the random seed to a repetition-specific value that is the same across\n",
    "    # all benchmark tasks and method\n",
    "    idx = task[0]\n",
    "    np.random.seed(583 * idx + idx)\n",
    "\n",
    "    benchmark_name = task[1]\n",
    "    benchmark_test_function_name = task[2]\n",
    "\n",
    "    benchmark = benchmarks[benchmark_name]\n",
    "    benchmark_test_function = benchmark_test_functions[benchmark_test_function_name]\n",
    "    res = benchmark(benchmark_test_function)\n",
    "\n",
    "    return (benchmark_name, benchmark_test_function_name, idx, res[0], res[1])\n",
    "\n",
    "def run_benchmarks(benchmarks, benchmark_test_functions, repeat=50):\n",
    "    tasks = []\n",
    "    for benchmark in benchmarks.keys():\n",
    "        for benchmark_test_function in benchmark_test_functions.keys():\n",
    "            for i in range(repeat):\n",
    "                tasks.append((i, benchmark, benchmark_test_function))\n",
    "\n",
    "    p = multiprocessing.Pool()\n",
    "    return p.map(run_task, tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of neurons in each population\n",
    "N = 100\n",
    "\n",
    "# Model coefficients obtained from running the empirical conductance-based synapse neuron\n",
    "w_cond =  np.array((\n",
    "     3.51649857e+00,\n",
    "     2.33295706e+05,\n",
    "    -1.39254970e+05,\n",
    "     1.00000000e+00,\n",
    "     2.24264021e+04,\n",
    "     4.21362391e+04))\n",
    "\n",
    "# Synaptic interaction weights corresponding to linear synaptic interaction\n",
    "w_cur = np.array((0.0, 1.0, -1.0, 100.0, 0, 0))\n",
    "\n",
    "# List of benchmarks to execute\n",
    "benchmarks = {\n",
    "    #\"cond_synapses\": lambda φ: benchmark_cond_synapses_no_intermediate(φ, w_cond, N),\n",
    "    #\"cur_synapses_no_intermediate\": lambda φ: benchmark_cond_synapses_no_intermediate(φ, w_cur, N),\n",
    "    #\"cur_synapses_intermediate_lstsq\": lambda φ: benchmark_cur_synapses_intermediate(φ, N),\n",
    "    \"cur_synapses_intermediate_backprop\": lambda φ: benchmark_cur_synapses_intermediate(φ, N, backprop=True),\n",
    "}\n",
    "\n",
    "# List of benchmark target functions\n",
    "benchmark_test_functions = {\n",
    "    \"addition\": lambda x, y: 0.5 * (x + y),\n",
    "    \"multiplication\": lambda x, y: x * y,\n",
    "    \"shunting\": lambda x, y: x / (1 + 10 * y),\n",
    "    \"norm\": lambda x, y: np.sqrt(x * x + y * y) / np.sqrt(2),\n",
    "    \"arctan\": lambda x, y: np.arctan2(x, y) / (0.5 * np.pi),\n",
    "    \"half-max\": lambda x, y: x * (x > y)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res = run_benchmarks(benchmarks, benchmark_test_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "with open('results_' + datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\") + '.json', 'w') as outfile:\n",
    "    json.dump(res, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyse_results(results):\n",
    "    # Reorganise the data\n",
    "    data = {}\n",
    "    for res in results:\n",
    "        if not res[1] in data:\n",
    "            data[res[1]] = {}\n",
    "        if not res[0] in data[res[1]]:\n",
    "            data[res[1]][res[0]] = {\n",
    "                \"optimized\": [],\n",
    "                \"reference\": []\n",
    "            }\n",
    "        data[res[1]][res[0]][\"optimized\"].append(res[3])\n",
    "        data[res[1]][res[0]][\"reference\"].append(res[4])\n",
    "\n",
    "    # Calculate mean and standard deviation, construct result tables\n",
    "    res = {}\n",
    "    for test_name, test in data.items():\n",
    "        for method_name, method in test.items():\n",
    "            for key, arr in method.items():\n",
    "                if not (key + \"_mean\") in res:\n",
    "                    res[key + \"_mean\"] = {}\n",
    "                    res[key + \"_stddev\"] = {}\n",
    "                if not test_name in res[key + \"_mean\"]:\n",
    "                    res[key + \"_mean\"][test_name] = {}\n",
    "                    res[key + \"_stddev\"][test_name] = {}\n",
    "                res[key + \"_mean\"][test_name][method_name] = np.mean(arr)\n",
    "                res[key + \"_stddev\"][test_name][method_name] = np.sqrt(np.var(arr))                \n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "tables = analyse_results(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pandas.DataFrame(tables[\"optimized_mean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pandas.DataFrame(tables[\"optimized_stddev\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pandas.DataFrame(tables[\"reference_mean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pandas.DataFrame(tables[\"reference_stddev\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

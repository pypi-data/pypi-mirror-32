[![Build Status](https://travis-ci.com/keon/toxic.svg?token=uEnEPs1KNePMw7bdrXQq&branch=master)](https://travis-ci.org/keon/toxic)
[![license](https://img.shields.io/github/license/mashape/apistatus.svg)](LICENSE)

<div align="center">
  <img width="35%" src="docs/source/_static/img/toxic.png"><br><br>
</div>

## Intro

**Toxic** is an open source software library for machine learning security.
It contains tools for adversarial example generation and provides a framework
for building new types of attack methods.

Currently in the dev stage.

## Attacks

Available attack algorithms implemented in Toxic:

* Fast Gradient Methods (FGM/FGSM) [`Tutorial`](/tutorial/source/fgsm.ipynb)
* Basic Iterative [`Tutorial`](/tutorial/source/basic_iterative.ipynb)
* Momentum Iterative [`Tutorial`](/tutorial/source/momentum_iterative.ipynb)
* DeepFool
* Universal Adversarial Perturbation (UAP)
* Jacobian-based Saliency Map Approach (JSMA)
* One Pixel Attack
* LBFGS
* Carlini Wagner L2
* Carlini Wagner L-inf
* Feature Adversaries
* Boundary Attack
* Elastic Net
* Natural Adversarial Examples (NAE)


### The Team

Toxic is a community driven project.
The project was initiated by machine learning security team @
[KakaoBrain](kakaobrain.com).

Metadata-Version: 2.1
Name: toxic
Version: 0.1.0
Summary: TODO
Home-page: https://github.com/KakaoBrain
Author: KakaoBrain
Author-email: kwk236@nyu.edu
License: MIT License
Keywords: machine learning security adversarial attackdefense deep learning pytorch
Platform: UNKNOWN
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Education
Classifier: Intended Audience :: Science/Research
Classifier: Operating System :: POSIX :: Linux
Classifier: Operating System :: MacOS :: MacOS X
Classifier: Programming Language :: Python :: 3.4
Provides-Extra: profile
Provides-Extra: visualization
Provides-Extra: test
Provides-Extra: notebooks
Provides-Extra: dev
Requires-Dist: numpy (>=1.7)
Requires-Dist: scipy (>=0.19.0)
Requires-Dist: torch
Requires-Dist: six (>=1.10.0)
Provides-Extra: dev
Requires-Dist: torchvision; extra == 'dev'
Requires-Dist: flake8; extra == 'dev'
Requires-Dist: yapf; extra == 'dev'
Requires-Dist: isort; extra == 'dev'
Requires-Dist: pytest; extra == 'dev'
Requires-Dist: pytest-xdist; extra == 'dev'
Requires-Dist: nbval; extra == 'dev'
Requires-Dist: nbstripout; extra == 'dev'
Requires-Dist: pypandoc; extra == 'dev'
Requires-Dist: sphinx; extra == 'dev'
Requires-Dist: sphinx-rtd-theme; extra == 'dev'
Provides-Extra: notebooks
Requires-Dist: jupyter (>=1.0.0); extra == 'notebooks'
Provides-Extra: profile
Requires-Dist: prettytable; extra == 'profile'
Provides-Extra: test
Requires-Dist: pytest; extra == 'test'
Requires-Dist: pytest-cov; extra == 'test'
Requires-Dist: nbval; extra == 'test'
Requires-Dist: visdom; extra == 'test'
Requires-Dist: torchvision; extra == 'test'
Provides-Extra: visualization
Requires-Dist: matplotlib (>=1.3); extra == 'visualization'
Requires-Dist: visdom (>=0.1.4); extra == 'visualization'
Requires-Dist: pillow; extra == 'visualization'


.. raw:: html

   <div align="center">

.. raw:: html

   </div>

Intro
-----

**Toxic** is an open source software library for machine learning
security. It contains tools for adversarial example generation and
provides a framework for building new types of attack methods.

Currently in the dev stage.

Attacks
-------

Available attack algorithms implemented in Toxic:

-  Fast Gradient Methods (FGM/FGSM)
   ```Tutorial`` </tutorial/source/fgsm.ipynb>`__
-  Basic Iterative
   ```Tutorial`` </tutorial/source/basic_iterative.ipynb>`__
-  Momentum Iterative
   ```Tutorial`` </tutorial/source/momentum_iterative.ipynb>`__
-  DeepFool
-  Universal Adversarial Perturbation (UAP)
-  Jacobian-based Saliency Map Approach (JSMA)
-  One Pixel Attack
-  LBFGS
-  Carlini Wagner L2
-  Carlini Wagner L-inf
-  Feature Adversaries
-  Boundary Attack
-  Elastic Net
-  Natural Adversarial Examples (NAE)

The Team
~~~~~~~~

Toxic is a community driven project. The project was initiated by
machine learning security team @ `KakaoBrain <kakaobrain.com>`__.

.. |license| image:: https://img.shields.io/github/license/mashape/apistatus.svg



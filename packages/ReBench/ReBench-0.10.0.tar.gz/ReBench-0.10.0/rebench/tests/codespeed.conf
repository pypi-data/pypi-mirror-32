# Config file for ReBench
# Config format is YAML (see http://yaml.org/ for detailed spec)

# this run definition will be choosen if no parameters are given to rebench.py
standard_experiment: Test
standard_data_file: 'codespeed.data'

# reporting should enable the configuration of the format of the out put
# REM: not implement yet (STEFAN: 2011-01-19)
reporting:
    # results can also be reported to a codespeed instance
    # see: https://github.com/tobami/codespeed
    codespeed:
        url: http://localhost:1/ # not supposed to work 
        # other details like commitid are required to be given as parameters
    csv_file: test.csv
    csv_locale: de_DE.UTF-8

runs:
    number_of_data_points: 10

# settings for quick runs, useful for fast feedback during experiments
quick_runs:
    number_of_data_points: 3
    max_time: 60   # time in seconds

# definition of benchmark suites
benchmark_suites:
    TestSuite1:
        gauge_adapter: TestVM
        # location: /Users/...
        command: TestBenchMarks %(benchmark)s %(input)s %(variable)s
        input_sizes: [2, 10]
        benchmarks:
            - Bench1:
                codespeed_name: "Bench1 with fancy name %(cores)s %(input_sizes)s %(extra_args)s "
            - Bench2:
                extra_args: 6

# VMs have a name and are specified by a path and the binary to be executed
virtual_machines:
    TestRunner1:
        path: tests
        binary: test-vm1.py %(cores)s
        cores: [1, 4]
        
# define the benchmarks to be executed for a re-executable benchmark run
experiments:
    Test:
        description: >
            This run definition is used for testing.
        benchmark: TestSuite1
        number_of_data_points: 1
        executions: TestRunner1

